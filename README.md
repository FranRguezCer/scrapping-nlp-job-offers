# üìä Data Jobs in Spain ‚Äì End-to-End Analysis with Web Scraping & NLP

[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tu_usuario/NLP-JOB-OFFERS/blob/main/notebooks/EDA_jobs.ipynb)

This project is an **end-to-end pipeline** that scrapes and analyzes job offers for data-related roles in Spain using **real listings from [Tecnoempleo](https://www.tecnoempleo.com/)**.

It combines:
- ‚úÖ Web scraping with Python to collect job market data
- ‚úÖ Exploratory Data Analysis (EDA) to identify trends
- ‚úÖ Rule-based NLP with `spaCy` to extract structured information (tech skills, soft skills, work mode)
- ‚úÖ A dual comparison of keyword matching vs NLP pattern matching

> Ideal for aspiring **Data Analysts** and **Junior Data Scientists** aiming to understand real job market expectations in 2025.

---

## üöÄ Project Highlights

- üîé Scrapes 150 real job offers related to *data roles* (e.g. Data Analyst, Data Scientist, Data Engineer)
- üåê Handles listings in both **Spanish and English**
- üß† Applies multilingual NLP (`spaCy`) to detect soft/technical skills and work modality
- üìä Compares results from basic keyword matching and smart rule-based NLP
- üßº Cleans, structures and enriches raw text data into actionable insights

---

## üì¶ Project Structure

```bash
data-jobs-analysis/
‚îÇ
‚îú‚îÄ‚îÄ data/                            # All raw and processed datasets
‚îÇ   ‚îú‚îÄ‚îÄ job_offers_list.csv          # Listings scraped from Tecnoempleo (title, company, URL)
‚îÇ   ‚îî‚îÄ‚îÄ job_offers_detailed.csv      # Enriched offers with descriptions, metadata, tech tags
‚îÇ
‚îú‚îÄ‚îÄ scraper/                         # Web scraping scripts
‚îÇ   ‚îú‚îÄ‚îÄ scrape_listings.py           # Scrapes multiple pages of job listings
‚îÇ   ‚îî‚îÄ‚îÄ scrape_offer_details.py      # Extracts detailed info from each job URL
‚îÇ
‚îú‚îÄ‚îÄ notebooks/                       # Analysis and visualization notebooks
‚îÇ   ‚îî‚îÄ‚îÄ EDA_jobs.ipynb               # Exploratory data analysis and feature engineering
‚îÇ
‚îú‚îÄ‚îÄ requirements.txt                 # List of Python dependencies
‚îú‚îÄ‚îÄ README.md                        # Project documentation and summary
‚îî‚îÄ‚îÄ .gitignore                       # Files and folders to ignore in version control
```

---

## üß∞ Technologies Used

- **Python** ‚Äì main programming language
  - `requests`, `BeautifulSoup` ‚Äì for HTML parsing and web scraping
  - `pandas`, `re`, `collections` ‚Äì for structured data manipulation and keyword extraction
  - `matplotlib`, `seaborn` ‚Äì for data visualization and comparisons
  - `spaCy` ‚Äì for NLP-based skill extraction in both **English** and **Spanish**
    - Language models used: `en_core_web_md` and `es_core_news_md`
    - `PhraseMatcher` for multilingual rule-based entity detection
  - `Jupyter Notebook` / `Google Colab` ‚Äì for interactive and cloud-based analysis

---

## üîß Setup Instructions

### 1. Clone the repository
```bash
git clone https://github.com/FranRguezCer/scrapping-nlp-job-offers.git
cd scrapping-nlp-job-offers
```

### 2. (Optional but recommended) Create and activate a virtual environment
```bash
python -m venv venv-nlp
source venv-nlp/bin/activate         # Windows: .\venv-nlp\Scripts\activate
```

### 3. Install Python dependencies
```bash
pip install -r requirements.txt
```

### 4. Download spaCy language models
```bash
python -m spacy download en_core_web_md
python -m spacy download es_core_news_md
```

---

## üìä Dataset Summary

- ‚úÖ Offers collected with the keyword: `"data"`
- ‚úÖ Scraped from 5+ pages of search results (a total of 150 job offers from [Tecnoempleo](tecnoempleo.com))
- ‚úÖ Full details extracted per offer: title, company, location, description, publication date, contract type, etc.
- ‚úÖ Technologies auto-detected from free-text job descriptions (Python, SQL, Power BI, etc.)

---

## üï∏Ô∏è Web Scraping Process

The job offers were scraped from [Tecnoempleo](https://www.tecnoempleo.com/), a major Spanish tech job board.  
The scraping pipeline consists of two steps:

1. **Listing Scraper (`scrape_listings.py`)**  
   - Crawls multiple pages and collects basic info (job title, company, and URL to each job offer).

2. **Details Scraper (`scrape_offer_details.py`)**  
   - Visits each job URL to extract full descriptions, contract type, work modality, required skills, and more.

Both scripts use `requests` + `BeautifulSoup`, and results are saved into `job_offers_list.csv` and `job_offers_detailed.csv`.

---

## üìì Try the EDA and NLP analysis in Google Colab

The main notebook includes all preprocessing, skill detection, and comparisons between keyword-based and NLP-based extractions.

> ‚ö° No installation needed ‚Äî run it directly in the cloud:

[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/FranRguezCer/scrapping-nlp-job-offers/blob/main/notebooks/EDA_jobs.ipynb)

---

## üß† Future Work

- Applying Named Entity Recognition (NER) or transformer-based models to enrich the analysis.
- Extracting salary ranges or required years of experience using NLP.
- Automating updates to track how trends evolve over time.

---

## üë®‚Äçüíª Author

> Created by **[Francisco Jos√© Rodr√≠guez Cerezo]**, aspiring data analyst with a passion for real-world applications and strong technical curiosity.  
> Currently enhancing my portfolio with practical end-to-end projects.

[LinkedIn](https://linkedin.com/in/franciscojoserodriguezcerezo) | [Portfolio](https://franrguezcer.github.io/portfolio/) | [Email](mailto:fjrguezcerezo@gmail.com)

---

## üìù License

This project is licensed under the [MIT License](LICENSE).

You are free to use, modify, and distribute this code for personal or commercial purposes, provided that proper credit is given.  
This software is provided **"as is"**, without warranty of any kind.

¬© 2025 Francisco Jos√© Rodr√≠guez Cerezo

